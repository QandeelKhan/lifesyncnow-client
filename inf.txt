Open the terminal.
also create a PAT personal access token from the settings of the account
Type the following command to set your username in Git:

python

git config --global user.name "Your_Username"

Type the following command to set your email in Git:

python

git config --global user.email "your_email@example.com"

To verify the configuration, type:

lua

git config --global user.name

and

lua

    git config --global user.email

This sets up Git to use the same username and email for all your projects. You can change the configuration for a specific project by using the git config command without the --global flag.

-----
svgr example: npx @svgr/cli -- src/components/icons/UserIcon.svg
execute python3 script: python3 ./<filename>.py


about project
we added the object ("browser": {
    "fs": false,
    "path": false,
    "os": false
})  
to the package.json, to fix typescript v5 while importing fs(file system module) else downgrade to v4 until not fixed in v5
-------

Yes, you can store your GitHub username and personal access token as environment variables in your system, and then reference them in your commands. This way, you don't have to type your credentials every time you want to use the GitHub API.

Here's an example using cURL:

    First, you'll need to generate a personal access token from the GitHub Developer Settings.

    Then, add the following lines to your shell profile file (e.g. ~/.bashrc or ~/.zshrc) and replace <username> with your GitHub username and <token> with your personal access token:

javascript

export GH_USERNAME=<username>
export GH_TOKEN=<token>

    Restart your terminal or run the following command to reload your profile:

bash

source ~/.bashrc

    Now, you can create a repository by running the following command, replacing <repo_name> with the name of your repository:

bash cmd to create a repository on github

curl -u $GH_USERNAME:$GH_TOKEN https://api.github.com/user/repos -d '{"name":"<repo_name>"}'

This will use the environment variables to access your GitHub credentials, so you don't have to type them every time.
# ---------To push changes from the main branch to the backup branch, 
you need to checkout the backup branch and merge the changes from the main branch into it. 
Here's the steps to do that in the terminal:

    Checkout the main branch: git checkout main
    Fetch the latest changes from the remote repository: git fetch
    Merge the changes from the main branch into the backup branch: git checkout backup-branch && git merge main
    Push the changes to the remote repository: git push

Note: Replace main and backup-branch with the actual names of your branches.
# ---------To push changes from the main branch to the backup branch, 
sudo fuser -k 8000/tcp

# AVAILABLE BRANDINGS:
symbolize
1: HeathHive
2: WellnessZoneX
3: HealthFlare
4: FitnessFusion
5: HealthyHype
6: WellnessVerse
7: healthyVerse
8: healthyHub

our application is single, not the cluster based becuase we needed to use kubernettes and docker-swarm for that, which i have no knowledge for them.
digitalocean token for life-sync-now: dop_v1_dd66186ffe4d81e9e70f315f78ce677a59c0879c8bce4126f546ede24ae914e8

‚ùØ docker-machine create(rather then "create" we can use "rm --f life-sync-now-server" to remove this machine) \ 
        --driver digitalocean \
        --digitalocean-access-token dop_v1_dd66186ffe4d81e9e70f315f78ce677a59c0879c8bce4126f546ede24ae914e8 \
        # --engine-install-url "https://releases.rancher.com/install-docker/19.03.9.sh" \ (only add this line in case of any error or problem)
        --digitalocean-image ubuntu-22-10-x64 \
        life-sync-now-server(as a server name, we can call it whatever);
after that we can see a droplet created with the name of life-sync-now-server, a droplet on digitalocean means a server.
To see how to connect your Docker Client to the Docker Engine running on this virtual machine, 
run: docker-machine env life-sync-now-server
cmd:"docker-machine ls" (to see the available created machines names and ip info on our computer)
as now we can access the machine ip we can use ssh(secure shell(a protocol to connect with servers)) to connect with our server by opening a secure shell.
the beauty of docker machine is that we donot have to create the ssh manually and hide all of that complexity of creating and ssh.
cmd: "docker-machine ssh life-sync-now-server" (so we can simply type this followed to logged-in(connect) to to our machine using ssh, and all of this was setup when we created this docker machine(great job), after loging in into ssh we can type "cd /" and press ls to see all the directories of our ubuntu os)
to run the production compose file: 
docker-compose -f docker-compose.prod.yml build
after doing all the shit mentioned we finally have to run "docker-compose -f docker-compose.prod.yml up -d"
this will gona build our image on the server we created earlier with the docker-machine.
"docker-compose -f docker-compose.prod.yml up -d --biuld"

"docker-machine regenerate-certs life-sync-now-server"(sometime require for docker-machine whille deploying because of not connecting to the server, so then aga)
# SERVE DJANGO THROUGH GUNICRON PROD SERVER:
1: Correct settings environment to load:
Make sure to run server on which settings.py like dev.py or the prod.py and set it in the manage.py, wsgi.py, asgi.py file.
so both reference the same settings environment, i.e OurBlogBackend.settings.dev or OurBlogBackend.settings.prod.
2: Run guniron to serve django application: "gunicorn OurBlogBackend.wsgi:application"


# BIGGEST TIME WASTING PROBLEMS ENCOUNTERED SO FAR THAT TAKES AND RUINED UP SO MANY DAYS WITHOUT ANY BIG REASONS:
1: AWS S3: 
Always add the bucket policy on the bucket, if using the development environment of django where we have only local http server, 
that's why the request will be 403 forbiddent to the s3 bucket and assets will not retrieve, this is only problem in development environment only, 
but it's better to add the policy to accept all requests to not get more errors.
2:
Decision making and making changes on docker different type of configs with multiple aspects and experiments each times. 
Every goodness comes with some cost.Don't be over complex everytime.
3: Sometimes Django cannot resolve the databse host address and gives errors regardles of dns resolving(because of ip of container) like below:
"RuntimeWarning: Got an error checking a consistent migration history performed for database connection 'default': could not translate host name "db1"" (if we set the host name to db1 or db2 etc whatever it will not resolve it)
SOLUTOIN:
The solution i was eneded up, got from stacks overflow somehow, by chance. was to delete the images and volumes related to our services that give error, 
in my case it was django and postgres images and volumes previously cached some files on server causing the error even thouh i run the services using --build command.
